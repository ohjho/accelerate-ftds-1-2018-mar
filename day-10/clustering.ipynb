{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering\n",
    "\n",
    "* Identify the regions in which data is concentrated by seperating them into different groups or cluster.\n",
    "* The different clusters can help you spot relasionships hard to normally\n",
    "* Clustering is a type of unsupervised learning since we don't need labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "They have a host of applications including:\n",
    "\n",
    "* Topic Modelling (discovering topics in text)\n",
    "* Image segmentation (clustering pixels)\n",
    "* Customer Segmentation\n",
    "* Fraud Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Means\n",
    "\n",
    "K-Means is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The way the algorithm works  as follows.\n",
    "\n",
    "1. Randomly pick $ k $ points as our cluster centers, these points are called centroids\n",
    "2. Assgin each $ x_i $ to the closest centroid based on euclidean distance, to create a cluster.\n",
    "3. Take the mean of all points assigned to a cluster to calculate new centroid.\n",
    "4. Repeat step 2 and 3 until none of the centorids move or we pass the iteration limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/800/1*KrcZK0xYgTa4qFrVr0fO2w.gif\" align=\"middle\"  width=\"600\"> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*KrcZK0xYgTa4qFrVr0fO2w.gif\" align=\"middle\"  width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Advantages\n",
    "  * Works best with numerical data.\n",
    "  * Simple and fast.\n",
    "* Disadvantages\n",
    "  * Won't work we'll with non sphereical data. \n",
    "  * Doesn't work great with clusters of different density.\n",
    "  * Can be misled by outliers (K-medians is altervative thats more robust)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering can be split into 2 types:\n",
    "* Agglomerative  (bottom up)\n",
    "* Divisive (top down)\n",
    "\n",
    "We'll focus on the agglomerative kind.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does the algorithm work?\n",
    "\n",
    "* Initally we say that each point is a cluster\n",
    "* Then repeatdedly combine the two 'nearest' clusters into one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/800/1*ET8kCcPpr893vNZFs8j4xg.gif\" align=\"middle\"  width=\"1000\"> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*ET8kCcPpr893vNZFs8j4xg.gif\" align=\"middle\"  width=\"1000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to interpret the dendrogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://www.sthda.com/english/sthda-upload/figures/cluster-analysis/011-visualizing-dendrograms-cutree-1.png\" align=\"middle\"  width=\"700\"> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"http://www.sthda.com/english/sthda-upload/figures/cluster-analysis/011-visualizing-dendrograms-cutree-1.png\" align=\"middle\"  width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we define the nearest point? We need some kind of distace metric, there quite a few to choose from including:\n",
    "\n",
    "* Eucldiean\n",
    "* Manhattan\n",
    "* Cosine \n",
    "\n",
    "\n",
    "Eucldiean is the default in sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Eucldiean distance through a picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://rosalind.info/media/Euclidean_distance.png\" align=\"middle\"  width=\"700\"> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"http://rosalind.info/media/Euclidean_distance.png\" align=\"middle\"  width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "How do our distance metric work with multiple points in a cluster? On way is to take average for points in cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://cisrg.shef.ac.uk/voices/images/Centroid.jpg\" align=\"middle\"  width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two other way to merge clusters are single linkage (compare closest point in each cluster or complete linkage (compare furthest point in each cluster). Bellow is single linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cisrg.shef.ac.uk/voices/images/Slink.jpg\" align=\"middle\"  width=\"700\"> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"http://cisrg.shef.ac.uk/voices/images/Slink.jpg\" align=\"middle\"  width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "* Advantages\n",
    "  * Can be easily visulized with a dendrogram.\n",
    "* Disadvantages\n",
    "  * In general is more computation and memory heavy than other algorithms.\n",
    "  * Not very robust to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DBSCAN\n",
    "\n",
    "DBSCAN or density based clustering is  algorithm that groups together points that are closely packed or dense in space. The algorithm has two parameters, the minimun number of points for a cluster and $ \\varepsilon  $ the maximum radius of the neighboorhood, or how close a point needs to be to be considered directly reachable from another point.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<div  style=\"float: left\" >\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-fb7f9859bb53437d7b5fadf7363a0001-c\" align=\"middle\"  width=\"500\"> \n",
    "<ul>\n",
    "    <li>Core point – a point that has at least a minimum number of other points (minPts) within its ε radius.</li>\n",
    "    <li> Border point – a point is within the ε radius of a core point BUT has less than the minimum number of other points (minPts) within its own ε radius</li>\n",
    "    <li>Noise point – a point that is neither a core point or a border point</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Core point – a point that has at least a minimum number of other points (minPts) within its ε radius.\n",
    "* Border point – a point is within the ε radius of a core point BUT has less than the minimum number of other points (minPts) within its own ε radius\n",
    "* Noise point – a point that is neither a core point or a border point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Larger clusters are formed when directly reachable points are chain together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://qph.ec.quoracdn.net/main-qimg-f27d5b6849955529c01c59b9dda3435f\" align=\"middle\"  width=\"1000\"> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-f27d5b6849955529c01c59b9dda3435f\" align=\"middle\"  width=\"1000\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/800/1*tc8UF-h0nQqUfLC8-0uInQ.gif\" align=\"middle\"  width=\"1000\"> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*tc8UF-h0nQqUfLC8-0uInQ.gif\" align=\"middle\"  width=\"1000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Advantages\n",
    "  * Doesn't require you to specify the number of points\n",
    "  * Can find clusters in strangely shaped data\n",
    "  * Has a notion of noise and is robust to outliers\n",
    "\n",
    "* Disadvantages\n",
    "  * Bad if clusters don't have a similar density \n",
    "  * Can be sensitive to starting parameters making hard to tune.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
